{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Título del trabajo: Chromatic Charm\n",
    "\n",
    "#### 2. **Introducción:**\n",
    "\n",
    "La búsqueda de soluciones para la colorización precisa de imágenes en blanco y negro ha sido un desafío constante en el campo del procesamiento de imágenes y visión por computadora. En esta nueva iteración de nuestro proyecto, denominado \"Chromatic-Charm\", nos enfocamos en mejorar la calidad visual de imágenes monocromáticas mediante el uso de técnicas avanzadas de ciencia de datos y aprendizaje profundo. Exploraremos cómo podemos agregar color de manera realista y convincente a estas imágenes, abriendo nuevas posibilidades creativas y prácticas en el ámbito de la visualización digital.\n",
    "\n",
    "Este proceso no solo requiere comprender la estructura y los detalles de las imágenes, sino también capturar la esencia y la tonalidad adecuadas para que el resultado final sea convincente para el ojo humano. A través del uso de técnicas avanzadas de aprendizaje profundo, exploraremos cómo podemos lograr esta transformación de manera eficaz y eficiente, abriendo nuevas posibilidades creativas y prácticas en el mundo de la visualización digital.\n",
    "\n",
    "#### 3. **Antecedentes:**\n",
    "Este proyecto revisita investigaciones previas que abordaron la colorización de imágenes mediante la implementación de autoencoders convolucionales ([Image-Colorization](https://github.com/xagallegos/Image-Colorization)). Aunque estos métodos demostraron cierto grado de capacidad para representar y generar imágenes en color, se descubrió que los resultados obtenidos no cumplían con los estándares deseados en cuanto a claridad y calidad visual. Estas limitaciones nos llevan a la exploración de nuevas metodologías, en este caso las GANs.\n",
    "\n",
    "A lo largo de este trabajo se referencia el trabajo de Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). [Image-to-image translation with conditional adversarial networks](https://arxiv.org/pdf/1611.07004.pdf), quienes exploran las redes adversarias condicionales como una solución de propósito general para problemas de traducción de imagen a imagen. \n",
    "\n",
    "#### 4. **Objetivos:**\n",
    "\n",
    "**Objetivos Generales:**\n",
    "Aplicar conceptos avanzados de ciencia de datos y aprendizaje profundo para desarrollar un modelo que sea capaz de colorizar imágenes de forma efectiva, conservando los detalles y la calidad de las mismas.\n",
    "\n",
    "**Objetivos Específicos:**\n",
    "1. Encontrar y adaptar un dataset de imágenes en pares de blanco y negro y color.\n",
    "2. Realizar un exhaustivo análisis exploratorio de datos (EDA) para comprender las características y la distribución de las imágenes a blanco y negro, así como sus versiones a color.\n",
    "3. Preprocesar y limpiar las imágenes para eliminar artefactos y valores atípicos que puedan afectar la calidad del modelo de colorización.\n",
    "4. Entrenar modelos de Redes Generativas Adversarias (GANs) utilizando técnicas de ciencia de datos avanzadas para la colorización de imágnes. \n",
    "5. Implementar un sistema de registro de experimentos utilizando MLflow para rastrear el rendimiento de los modelos y optimizar los parámetros.\n",
    "6. Seleccionar el mejor modelo en función de métricas de rendimiento y despliegue del modelo en una infraestructura en la nube para su uso práctico.\n",
    "\n",
    "#### 5. **Planteamiento del problema:**\n",
    "El desafío consiste en desarrollar un modelo efectivo que pueda agregar color de manera precisa a imágenes en blanco y negro, preservando la calidad original, de manera que el resultado final sea capaz de pasar desapercibido como una imágen a color para el ojo humano.\n",
    "\n",
    "Para el entrenamiento del modelo, se eligió el conjunto de datos Places365 debido a su amplia diversidad de entornos, lo que se espera permita al modelo aprender características representativas de una amplia gama de escenarios y contribuya a la capacidad de generalización del modelo. Cabe destacar que, dado el considerable tamaño del conjunto de datos completo, se utilizará un subconjunto de más de 36,000 imágenes obtenidas de [Kaggle](https://www.kaggle.com/datasets/pankajkumar2002/places365), más alineado con los recursos asignados al proyecto.\n",
    "\n",
    "#### 6. **Desarrollo de la solución:**\n",
    "\n",
    "## **EDA (Análisis Exploratorio de Datos):** \n",
    "El análisis exploratorio de datos en nuestro proyecto fue esencialmente visual y descriptivo, enfocado en entender la naturaleza y la calidad de las imágenes proporcionadas por el dataset Places365. Este EDA nos permitió validar la adecuación de los datos para el modelo de aprendizaje profundo que pretendemos implementar. A continuación, describimos las principales actividades realizadas en esta etapa:\n",
    "\n",
    "1. **Visualización de Imágenes:** Se realizaron visualizaciones iniciales de las imágenes pertenecientes a los conjuntos de entrenamiento y prueba. Esto nos permitió tener una primera impresión visual de la variedad y la naturaleza de las imágenes que estaríamos manejando, lo cual es crucial para identificar posibles anomalías o características distintivas en los datos.\n",
    "\n",
    "2. **Revisión del Tamaño de las Imágenes:** Confirmamos que todas las imágenes tienen una resolución de 256x256 píxeles. Esta resolución es adecuada para nuestro proyecto ya que imágenes de alta resolución contienen más detalles y características visuales, lo que potencialmente mejora la capacidad del modelo para aprender patrones complejos y sutilezas en los datos.\n",
    "\n",
    "3. **Análisis del Rango de Valores de Píxeles:** Se examinó el rango de valores de píxeles en las imágenes para determinar si era necesario aplicar alguna normalización o ajuste. La uniformidad en el rango de píxeles es importante para evitar que el modelo desarrolle un sesgo hacia características menos significativas como el brillo o el contraste de las imágenes.\n",
    "\n",
    "Aunque el análisis fue relativamente directo, estas observaciones son fundamentales para asegurar que el dataset es apropiado para el tipo de modelo que deseamos entrenar, y que las imágenes son de calidad suficiente para no introducir errores o sesgos no deseados en las fases posteriores del proyecto.\n",
    "\n",
    "## **Data Wrangling:**\n",
    "\n",
    "Durante la fase de preparación y limpieza de datos, llevamos a cabo dos procesos principales para asegurar que los datos estuvieran en el formato adecuado para el entrenamiento del modelo. Estos procesos son esenciales para facilitar el aprendizaje del modelo y mejorar su eficacia al trabajar con imágenes. A continuación, describimos los pasos realizados:\n",
    "\n",
    "1. **Reducción de Dimensionalidad:** Convertimos las imágenes de color, que originalmente tenían tres canales (RGB), a escala de grises, quedando con un solo canal. Esta transformación es crucial porque nuestro objetivo es desarrollar un modelo que aprenda a colorear imágenes automáticamente. Reducir la dimensionalidad a un solo canal facilita el manejo de los datos y reduce la complejidad computacional, permitiendo que el modelo se enfoque en aprender la estructura y textura de las imágenes antes de aplicar color.\n",
    "\n",
    "2. **Normalización de Píxeles:** Normalizamos los valores de los píxeles de las imágenes dividiéndolos por 255. Este paso convierte el rango original de los píxeles, que es de 0 a 255, a un rango de 0 a 1. La normalización es una práctica estándar en el procesamiento de imágenes para modelos de aprendizaje automático, ya que mejora la convergencia durante el entrenamiento al mantener los valores de entrada dentro de una escala manejable y más uniforme.\n",
    "\n",
    "Estos ajustes nos aseguran que el dataset está preparado de manera óptima para el entrenamiento del modelo, facilitando un aprendizaje más efectivo y eficiente.\n",
    "\n",
    "\n",
    "## **Dataset final a trabajar:** \n",
    "Para el entrenamiento de nuestro modelo, seleccionamos el dataset Places365 debido a su amplia diversidad de entornos. Esta característica es fundamental, ya que esperamos que el modelo aprenda características representativas de una amplia gama de escenarios, lo cual es crucial para mejorar su capacidad de generalización.\n",
    "\n",
    "#### Selección del Subconjunto:\n",
    "Dado el considerable tamaño del dataset completo, optamos por utilizar un subconjunto que consta de más de 36,000 imágenes. Este subconjunto fue obtenido de Kaggle y fue seleccionado para estar más alineado con los recursos computacionales disponibles para el proyecto. La elección de este subconjunto asegura que podemos manejar eficientemente el volumen de datos durante el entrenamiento sin comprometer la diversidad de las imágenes, lo cual es esencial para el objetivo de entrenar un modelo robusto y generalizable.\n",
    "\n",
    "#### Justificación de la Selección:\n",
    "La elección de Places365 y específicamente del subconjunto de Kaggle se basa en la necesidad de equilibrar la calidad y diversidad del dataset con las limitaciones prácticas de nuestros recursos de hardware y tiempo. Esto nos permite aprovechar un conjunto de datos rico y variado, manteniendo la viabilidad del proyecto dentro de los marcos académicos y técnicos establecidos.\n",
    "\n",
    "## **Entrenamiento del modelo con MLflow:**\n",
    "\n",
    "El entrenamiento del modelo se gestionó utilizando MLflow, una plataforma abierta para la gestión del ciclo de vida completo de los modelos de machine learning. MLflow nos permitió registrar, versionar y controlar los experimentos realizados, facilitando la reproducibilidad y la trazabilidad de los modelos desarrollados.\n",
    "\n",
    "#### Registro de Experimentos:\n",
    "Utilizamos MLflow para registrar cada experimento, incluyendo detalles como la configuración de hiperparámetros. Cada experimento fue documentado con su correspondiente conjunto de parámetros y métricas para facilitar comparaciones y análisis detallados.\n",
    "\n",
    "#### Configuración de Modelos y Parámetros:\n",
    "Para cada modelo, configuramos y registramos múltiples hiperparámetros, tales como:\n",
    "\n",
    "- Tasa de aprendizaje para el generador (`lr_G`) y el discriminador (`lr_D`).\n",
    "- Número de épocas.\n",
    "- Betas para los optimizadores.\n",
    "\n",
    "#### Métricas de Rendimiento:\n",
    "Para evaluar la calidad de las imágenes generadas por nuestros modelos, empleamos la métrica Fréchet Inception Distance (FID). La FID mide la distancia en el espacio de características entre dos distribuciones de imágenes, en este caso, las generadas por el modelo y las reales del dataset. Un valor bajo de FID indica que las imágenes generadas son similares a las reales, lo cual es un indicador de la alta calidad del modelo.\n",
    "\n",
    "Estos parámetros y métricas fueron esenciales para afinar el rendimiento de los modelos y explorar diversas configuraciones en busca de la mejor eficiencia.\n",
    "\n",
    "## **Selección del mejor modelo:** \n",
    "La selección del mejor modelo se basó en el análisis meticuloso de las métricas de rendimiento registradas en MLflow para cada experimento. Utilizando la métrica Fréchet Inception Distance (FID) como indicador clave de la calidad de las imágenes generadas, identificamos el modelo que logró los resultados más prometedores.\n",
    "\n",
    "#### Análisis de Métricas:\n",
    "La tabla siguiente resume algunos de los resultados de FID para varios modelos entrenados, destacando cómo la configuración de hiperparámetros afecta la calidad de las imágenes generadas.\n",
    "\n",
    "| Modelo | lr_G   | lr_D   | FID          |\n",
    "|--------|--------|--------|--------------|\n",
    "| GANv2  | 0.0001 | 0.0003 | -0.001369282 |\n",
    "| GAN    | 0.0002 | 0.0002 | 0            |\n",
    "| GANv3  | 0.0003 | 0.0001 | -0.001435816 |\n",
    "\n",
    "#### Elección del Modelo:\n",
    "El modelo denominado \"GAN\" demostró ser superior, alcanzando un FID de 0, que indica una similitud casi perfecta entre las imágenes generadas y las imágenes reales del dataset. Este resultado sugiere que el modelo no solo es capaz de generar imágenes de alta calidad, sino que también logra capturar con precisión las características y texturas originales.\n",
    "\n",
    "#### Configuración del Modelo Seleccionado:\n",
    "Los parámetros del modelo \"GAN\" seleccionado son los siguientes:\n",
    "\n",
    "- **Beta1:** 0.5\n",
    "- **Beta2:** 0.999\n",
    "- **Epochs:** 5\n",
    "- **Lambda_L1:** 100.0\n",
    "- **lr_D:** 0.0002\n",
    "- **lr_G:** 0.0002\n",
    "\n",
    "Este conjunto de parámetros proporcionó un equilibrio óptimo entre eficiencia en el aprendizaje y calidad de las imágenes generadas, como lo evidencia el excelente valor de FID obtenido.\n",
    "\n",
    "Esta estructura ha sido fundamental para garantizar la calidad y aplicabilidad de nuestro modelo GAN en tareas de colorización de imágenes.\n",
    "\n",
    "\n",
    "\n",
    "+ **Servir el modelo (API) con el mejor desempeño:** \n",
    "\n",
    "Para poner en producción nuestro modelo de aprendizaje automático más efectivo, desarrollamos una API utilizando FastAPI, una herramienta poderosa y eficiente para crear interfaces web. La API permite a los usuarios interactuar con nuestro modelo GAN, que se especializa en la colorización de imágenes.\n",
    "\n",
    "#### Creación de la API:\n",
    "La API fue diseñada para ser simple pero funcional, proporcionando un punto de acceso a través del cual los usuarios pueden enviar imágenes en escala de grises y recibir versiones colorizadas en respuesta. La implementación se realizó en Python utilizando el marco FastAPI junto con Uvicorn como servidor ASGI para un rendimiento óptimo.\n",
    "\n",
    "#### Carga y Preparación de Imágenes:\n",
    "Al recibir una imagen a través de la API, aplicamos una transformación para ajustar su tamaño a 256x256 píxeles, el tamaño requerido por nuestro modelo. Las imágenes se procesan en escala de grises, normalizadas y luego alimentadas al modelo para la generación de la versión colorizada.\n",
    "\n",
    "#### Modelo en Funcionamiento:\n",
    "En el evento de inicio de la aplicación, cargamos los pesos preentrenados del modelo desde un archivo local. Esto se hace para asegurar que el modelo esté listo para hacer predicciones inmediatamente después de que la API comience a funcionar.\n",
    "\n",
    "\n",
    "## **Conteneirizar del servicio:** \n",
    "\n",
    "Para facilitar el despliegue y asegurar la consistencia entre diferentes entornos, nuestro servicio de colorización de imágenes fue contenerizado utilizando Docker. Docker nos permite encapsular la aplicación y todas sus dependencias en un contenedor, lo que simplifica el despliegue en cualquier plataforma de nube.\n",
    "\n",
    "#### Creación del Dockerfile:\n",
    "El proceso comenzó con la creación de un Dockerfile, que es un script de configuración utilizado por Docker para construir imágenes de contenedores automáticamente. El Dockerfile especifica una imagen base de Python 3.11 y configura el entorno necesario para ejecutar nuestra aplicación:\n",
    "\n",
    "- **Actualización del sistema y configuración de zona horaria:** Se actualiza el sistema y se configura la zona horaria a la Ciudad de México para asegurar que los registros de tiempo sean consistentes con la ubicación del servidor.\n",
    "\n",
    "- **Instalación de dependencias:** Se instalan todas las dependencias necesarias para ejecutar la API a partir de un archivo `requirements.txt`. Esto incluye frameworks como FastAPI y bibliotecas para procesamiento de imágenes.\n",
    "\n",
    "- **Exposición del puerto:** El puerto 5051 es expuesto para permitir el acceso al servicio de la API desde fuera del contenedor.\n",
    "\n",
    "- **Comando de ejecución:** Se especifica el comando para iniciar el servidor Uvicorn con nuestra aplicación de FastAPI, configurado para escuchar en el puerto 5051.\n",
    "\n",
    "## **Despliegue del servicio en la nube:**\n",
    "\n",
    "El servicio de colorización de imágenes se desplegó en la nube utilizando Amazon Web Services (AWS), aprovechando su robusta infraestructura y capacidad de escalabilidad. Este despliegue en la nube nos permite ofrecer el servicio de manera confiable y eficiente a usuarios de todo el mundo.\n",
    "\n",
    "\n",
    "#### 7. **Conclusiones:**\n",
    "### Conclusiones\n",
    "\n",
    "Este proyecto de colorización de imágenes utilizando redes generativas adversarias (GANs) ha sido una experiencia profundamente educativa y desafiante para nosotros, un equipo de tres estudiantes universitarios. A continuación, se resumen los resultados del proyecto, se destacan los logros, se discuten los desafíos enfrentados y se brindan recomendaciones para mejoras futuras.\n",
    "\n",
    "#### Resultados y Logros:\n",
    "- **Desarrollo de un Modelo Eficaz:** Implementamos con éxito un modelo de GAN que aprende a colorizar imágenes en escala de grises de manera precisa, reflejado en una baja puntuación en la métrica Fréchet Inception Distance (FID).\n",
    "- **Creación y Despliegue de una API Funcional:** Desarrollamos una API utilizando FastAPI que facilita la interacción de los usuarios con nuestro modelo, demostrando la aplicabilidad de nuestro trabajo en escenarios reales.\n",
    "- **Contenerización y Despliegue en la Nube:** Nuestro modelo fue eficazmente contenerizado y desplegado en AWS, garantizando su disponibilidad y escalabilidad para responder a las solicitudes de los usuarios en tiempo real.\n",
    "\n",
    "#### Desafíos Enfrentados:\n",
    "- **Selección de Métricas:** Inicialmente enfrentamos dificultades para elegir una métrica adecuada que pudiera evaluar efectivamente la calidad de las imágenes generadas por nuestro modelo. La selección de la métrica FID fue crucial para superar este obstáculo.\n",
    "- **Limitaciones de Recursos y Tiempo:** La cantidad de datos y el tiempo disponible para entrenamiento y evaluación fueron limitados. Sin embargo, el acceso a recursos computacionales avanzados como GPUs y Google Colab facilitó significativamente el entrenamiento del modelo.\n",
    "- **Optimización de Hiperparámetros:** El proceso de afinar los hiperparámetros fue complejo y requirió múltiples iteraciones para encontrar la configuración óptima.\n",
    "\n",
    "Este proyecto nos permitió aplicar conocimientos teóricos en un contexto práctico y nos proporcionó valiosas lecciones sobre el trabajo en equipo y la gestión de proyectos de tecnología. Obteniendo una mejor idea de lo que conlleva todo el proceso de proyectos de ciencia de datos\n",
    "\n",
    "#### 8. **Referencias:**\n",
    "Dataset: https://research.google.com/audioset/dataset/speech.html\n",
    "\n",
    "GAN's: https://arxiv.org/pdf/1406.2661\n",
    "\n",
    "Métrica FID: https://arxiv.org/pdf/2401.09603\n",
    "\n",
    "Los estudiantes deben proporcionar una lista de todas las fuentes, libros, documentos técnicos, sitios web y recursos utilizados durante el proyecto. Esto incluye cualquier bibliografía y recursos citados en el informe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: El proyecto se debe encontrar en un repositorio de `dagshub` que al mismo tiempo debe estar ligado a un repositorio en `github`.\n",
    "\n",
    "El repositorio debe tener la siguiente estructura\n",
    "\n",
    "```\n",
    "Proyecto de Ciencia de Datos\n",
    "├── Informe Escrito\n",
    "│   └── PrimerNombrePrimerApellido.ipynb\n",
    "├── Data\n",
    "│   ├── Raw Data (folder)\n",
    "│   └── Clean Data (folder)\n",
    "├── Entrenamiento\n",
    "│   └── training.ipynb\n",
    "├── EDA\n",
    "│   └── EDA.ipynb\n",
    "└── API\n",
    "    ├── app.py\n",
    "    ├── Dockerfile\n",
    "    └── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2 Presentación.\n",
    "> Recuerden que la nota del proyecto es mitad el trabajo, y mitad la presentación. Deben hacer una presentación en power point para presentar el trabajo en la clase. La presentación, además de llevar todos los componentes básicos descritos en el entregable, debe llevar una tabla de contenido.\n",
    ">\n",
    "> - Presentación: 15 minutos.\n",
    "> - Seguir estas recomendaciones:\n",
    "    > - https://www.ncsl.org/legislative-staff/lscc/tips-for-making-effective-powerpoint-presentations\n",
    "    > - https://www.trentu.ca/academicskills/how-guides/how-write-university/how-approach-any-assignment/creating-effective-powerpoint-slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Cristian Camilo Zapata Zuluaga.\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rafael Juarez Badillo Chavez, Xander Gallegos, Javier Figueroa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
