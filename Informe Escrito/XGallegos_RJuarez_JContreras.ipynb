{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Título del trabajo: Chromatic Charm\n",
    "\n",
    "#### 2. **Introducción:**\n",
    "\n",
    "La búsqueda de soluciones para la colorización precisa de imágenes en blanco y negro ha sido un desafío constante en el campo del procesamiento de imágenes y visión por computadora. En esta nueva iteración de nuestro proyecto, denominado \"Chromatic-Charm\", nos enfocamos en mejorar la calidad visual de imágenes monocromáticas mediante el uso de técnicas avanzadas de ciencia de datos y aprendizaje profundo. Exploraremos cómo podemos agregar color de manera realista y convincente a estas imágenes, abriendo nuevas posibilidades creativas y prácticas en el ámbito de la visualización digital.\n",
    "\n",
    "Este proceso no solo requiere comprender la estructura y los detalles de las imágenes, sino también capturar la esencia y la tonalidad adecuadas para que el resultado final sea convincente para el ojo humano. A través del uso de técnicas avanzadas de aprendizaje profundo, exploraremos cómo podemos lograr esta transformación de manera eficaz y eficiente, abriendo nuevas posibilidades creativas y prácticas en el mundo de la visualización digital.\n",
    "\n",
    "#### 3. **Antecedentes:**\n",
    "Este proyecto revisita investigaciones previas que abordaron la colorización de imágenes mediante la implementación de autoencoders convolucionales ([Image-Colorization](https://github.com/xagallegos/Image-Colorization)). Aunque estos métodos demostraron cierto grado de capacidad para representar y generar imágenes en color, se descubrió que los resultados obtenidos no cumplían con los estándares deseados en cuanto a claridad y calidad visual. Estas limitaciones nos llevan a la exploración de nuevas metodologías, en este caso las GANs.\n",
    "\n",
    "A lo largo de este trabajo se referencia el trabajo de Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). [Image-to-image translation with conditional adversarial networks](https://arxiv.org/pdf/1611.07004.pdf), quienes exploran las redes adversarias condicionales como una solución de propósito general para problemas de traducción de imagen a imagen. \n",
    "\n",
    "#### 4. **Objetivos:**\n",
    "\n",
    "**Objetivos Generales:**\n",
    "Aplicar conceptos avanzados de ciencia de datos y aprendizaje profundo para desarrollar un modelo que sea capaz de colorizar imágenes de forma efectiva, conservando los detalles y la calidad de las mismas.\n",
    "\n",
    "**Objetivos Específicos:**\n",
    "1. Encontrar y adaptar un dataset de imágenes en pares de blanco y negro y color.\n",
    "2. Realizar un exhaustivo análisis exploratorio de datos (EDA) para comprender las características y la distribución de las imágenes a blanco y negro, así como sus versiones a color.\n",
    "3. Preprocesar y limpiar las imágenes para eliminar artefactos y valores atípicos que puedan afectar la calidad del modelo de colorización.\n",
    "4. Entrenar modelos de Redes Generativas Adversarias (GANs) utilizando técnicas de ciencia de datos avanzadas para la colorización de imágnes. \n",
    "5. Implementar un sistema de registro de experimentos utilizando MLflow para rastrear el rendimiento de los modelos y optimizar los parámetros.\n",
    "6. Seleccionar el mejor modelo en función de métricas de rendimiento y despliegue del modelo en una infraestructura en la nube para su uso práctico.\n",
    "\n",
    "#### 5. **Planteamiento del problema:**\n",
    "El desafío consiste en desarrollar un modelo efectivo que pueda agregar color de manera precisa a imágenes en blanco y negro, preservando la calidad original, de manera que el resultado final sea capaz de pasar desapercibido como una imágen a color para el ojo humano.\n",
    "\n",
    "Para el entrenamiento del modelo, se eligió el conjunto de datos Places365 debido a su amplia diversidad de entornos, lo que se espera permita al modelo aprender características representativas de una amplia gama de escenarios y contribuya a la capacidad de generalización del modelo. Cabe destacar que, dado el considerable tamaño del conjunto de datos completo, se utilizará un subconjunto de más de 36,000 imágenes obtenidas de [Kaggle](https://www.kaggle.com/datasets/pankajkumar2002/places365), más alineado con los recursos asignados al proyecto.\n",
    "\n",
    "#### 6. **Desarrollo de la solución:**\n",
    "Esta es la parte central del informe y debe dividirse en varias subsecciones:\n",
    "\n",
    "+ **EDA (Análisis Exploratorio de Datos):** \n",
    "El análisis exploratorio de datos en nuestro proyecto fue esencialmente visual y descriptivo, enfocado en entender la naturaleza y la calidad de las imágenes proporcionadas por el dataset Places365. Este EDA nos permitió validar la adecuación de los datos para el modelo de aprendizaje profundo que pretendemos implementar. A continuación, describimos las principales actividades realizadas en esta etapa:\n",
    "\n",
    "1. **Visualización de Imágenes:** Se realizaron visualizaciones iniciales de las imágenes pertenecientes a los conjuntos de entrenamiento y prueba. Esto nos permitió tener una primera impresión visual de la variedad y la naturaleza de las imágenes que estaríamos manejando, lo cual es crucial para identificar posibles anomalías o características distintivas en los datos.\n",
    "\n",
    "2. **Revisión del Tamaño de las Imágenes:** Confirmamos que todas las imágenes tienen una resolución de 256x256 píxeles. Esta resolución es adecuada para nuestro proyecto ya que imágenes de alta resolución contienen más detalles y características visuales, lo que potencialmente mejora la capacidad del modelo para aprender patrones complejos y sutilezas en los datos.\n",
    "\n",
    "3. **Análisis del Rango de Valores de Píxeles:** Se examinó el rango de valores de píxeles en las imágenes para determinar si era necesario aplicar alguna normalización o ajuste. La uniformidad en el rango de píxeles es importante para evitar que el modelo desarrolle un sesgo hacia características menos significativas como el brillo o el contraste de las imágenes.\n",
    "\n",
    "Aunque el análisis fue relativamente directo, estas observaciones son fundamentales para asegurar que el dataset es apropiado para el tipo de modelo que deseamos entrenar, y que las imágenes son de calidad suficiente para no introducir errores o sesgos no deseados en las fases posteriores del proyecto.\n",
    "\n",
    "+ **Data Wrangling:**\n",
    "\n",
    "Durante la fase de preparación y limpieza de datos, llevamos a cabo dos procesos principales para asegurar que los datos estuvieran en el formato adecuado para el entrenamiento del modelo. Estos procesos son esenciales para facilitar el aprendizaje del modelo y mejorar su eficacia al trabajar con imágenes. A continuación, describimos los pasos realizados:\n",
    "\n",
    "1. **Reducción de Dimensionalidad:** Convertimos las imágenes de color, que originalmente tenían tres canales (RGB), a escala de grises, quedando con un solo canal. Esta transformación es crucial porque nuestro objetivo es desarrollar un modelo que aprenda a colorear imágenes automáticamente. Reducir la dimensionalidad a un solo canal facilita el manejo de los datos y reduce la complejidad computacional, permitiendo que el modelo se enfoque en aprender la estructura y textura de las imágenes antes de aplicar color.\n",
    "\n",
    "2. **Normalización de Píxeles:** Normalizamos los valores de los píxeles de las imágenes dividiéndolos por 255. Este paso convierte el rango original de los píxeles, que es de 0 a 255, a un rango de 0 a 1. La normalización es una práctica estándar en el procesamiento de imágenes para modelos de aprendizaje automático, ya que mejora la convergencia durante el entrenamiento al mantener los valores de entrada dentro de una escala manejable y más uniforme.\n",
    "\n",
    "Estos ajustes nos aseguran que el dataset está preparado de manera óptima para el entrenamiento del modelo, facilitando un aprendizaje más efectivo y eficiente.\n",
    "\n",
    "\n",
    "+ **Dataset final a trabajar:** \n",
    "Para el entrenamiento de nuestro modelo, seleccionamos el dataset Places365 debido a su amplia diversidad de entornos. Esta característica es fundamental, ya que esperamos que el modelo aprenda características representativas de una amplia gama de escenarios, lo cual es crucial para mejorar su capacidad de generalización.\n",
    "\n",
    "#### Selección del Subconjunto:\n",
    "Dado el considerable tamaño del dataset completo, optamos por utilizar un subconjunto que consta de más de 36,000 imágenes. Este subconjunto fue obtenido de Kaggle y fue seleccionado para estar más alineado con los recursos computacionales disponibles para el proyecto. La elección de este subconjunto asegura que podemos manejar eficientemente el volumen de datos durante el entrenamiento sin comprometer la diversidad de las imágenes, lo cual es esencial para el objetivo de entrenar un modelo robusto y generalizable.\n",
    "\n",
    "#### Justificación de la Selección:\n",
    "La elección de Places365 y específicamente del subconjunto de Kaggle se basa en la necesidad de equilibrar la calidad y diversidad del dataset con las limitaciones prácticas de nuestros recursos de hardware y tiempo. Esto nos permite aprovechar un conjunto de datos rico y variado, manteniendo la viabilidad del proyecto dentro de los marcos académicos y técnicos establecidos.\n",
    "\n",
    "+ **Entrenamiento del modelo con MLflow:** En esta parte, se detalla cómo se entrenó el modelo de ciencia de datos. Deben utilizar MLflow para registrar los experimentos, incluyendo diferentes configuraciones de modelos y parámetros. Además, deben grabar métricas relevantes para evaluar el rendimiento de los modelos.\n",
    "\n",
    "\n",
    "\n",
    "+ **Selección del mejor modelo:** \n",
    "La selección del mejor modelo se basó en el análisis meticuloso de las métricas de rendimiento registradas en MLflow para cada experimento. Utilizando la métrica Fréchet Inception Distance (FID) como indicador clave de la calidad de las imágenes generadas, identificamos el modelo que logró los resultados más prometedores.\n",
    "\n",
    "#### Análisis de Métricas:\n",
    "La tabla siguiente resume algunos de los resultados de FID para varios modelos entrenados, destacando cómo la configuración de hiperparámetros afecta la calidad de las imágenes generadas.\n",
    "\n",
    "| Modelo | lr_G   | lr_D   | FID          |\n",
    "|--------|--------|--------|--------------|\n",
    "| GANv2  | 0.0001 | 0.0003 | -0.001369282 |\n",
    "| GAN    | 0.0002 | 0.0002 | 0            |\n",
    "| GANv3  | 0.0003 | 0.0001 | -0.001435816 |\n",
    "\n",
    "#### Elección del Modelo:\n",
    "El modelo denominado \"GAN\" demostró ser superior, alcanzando un FID de 0, que indica una similitud casi perfecta entre las imágenes generadas y las imágenes reales del dataset. Este resultado sugiere que el modelo no solo es capaz de generar imágenes de alta calidad, sino que también logra capturar con precisión las características y texturas originales.\n",
    "\n",
    "#### Configuración del Modelo Seleccionado:\n",
    "Los parámetros del modelo \"GAN\" seleccionado son los siguientes:\n",
    "\n",
    "- **Beta1:** 0.5\n",
    "- **Beta2:** 0.999\n",
    "- **Epochs:** 5\n",
    "- **Lambda_L1:** 100.0\n",
    "- **lr_D:** 0.0002\n",
    "- **lr_G:** 0.0002\n",
    "\n",
    "Este conjunto de parámetros proporcionó un equilibrio óptimo entre eficiencia en el aprendizaje y calidad de las imágenes generadas, como lo evidencia el excelente valor de FID obtenido.\n",
    "\n",
    "Esta estructura ha sido fundamental para garantizar la calidad y aplicabilidad de nuestro modelo GAN en tareas de colorización de imágenes.\n",
    "\n",
    "\n",
    "\n",
    "+ **Servir el modelo (API) con el mejor desempeño:** Describe cómo se creó una API para servir el modelo con el mejor rendimiento y métricas a través de una interfaz web o un servicio.\n",
    "\n",
    "+ **Conteneirizar del servicio:** Explica cómo se empaquetó el servicio y se subió a una plataforma de nube, lo que puede incluir el uso de contenedores como Docker.\n",
    "\n",
    "+ **Despliegue del servicio en la nube:** Detalla cómo se implementó el servicio en la nube, cómo se configuraron los recursos. La API se despliega utilizando el modelo que ha mostrado el mejor rendimiento y métricas durante los experimentos.\n",
    "\n",
    "\n",
    "#### 7. **Conclusiones:**\n",
    "En esta sección, los estudiantes deben resumir los resultados del proyecto, destacar los logros, discutir los desafíos enfrentados y brindar recomendaciones o posibles mejoras futuras.\n",
    "\n",
    "#### 8. **Referencias:**\n",
    "Dataset: https://research.google.com/audioset/dataset/speech.html\n",
    "\n",
    "GAN's: https://arxiv.org/pdf/1406.2661\n",
    "\n",
    "Métrica FID: https://arxiv.org/pdf/2401.09603\n",
    "\n",
    "Los estudiantes deben proporcionar una lista de todas las fuentes, libros, documentos técnicos, sitios web y recursos utilizados durante el proyecto. Esto incluye cualquier bibliografía y recursos citados en el informe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: El proyecto se debe encontrar en un repositorio de `dagshub` que al mismo tiempo debe estar ligado a un repositorio en `github`.\n",
    "\n",
    "El repositorio debe tener la siguiente estructura\n",
    "\n",
    "```\n",
    "Proyecto de Ciencia de Datos\n",
    "├── Informe Escrito\n",
    "│   └── PrimerNombrePrimerApellido.ipynb\n",
    "├── Data\n",
    "│   ├── Raw Data (folder)\n",
    "│   └── Clean Data (folder)\n",
    "├── Entrenamiento\n",
    "│   └── training.ipynb\n",
    "├── EDA\n",
    "│   └── EDA.ipynb\n",
    "└── API\n",
    "    ├── app.py\n",
    "    ├── Dockerfile\n",
    "    └── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2 Presentación.\n",
    "> Recuerden que la nota del proyecto es mitad el trabajo, y mitad la presentación. Deben hacer una presentación en power point para presentar el trabajo en la clase. La presentación, además de llevar todos los componentes básicos descritos en el entregable, debe llevar una tabla de contenido.\n",
    ">\n",
    "> - Presentación: 15 minutos.\n",
    "> - Seguir estas recomendaciones:\n",
    "    > - https://www.ncsl.org/legislative-staff/lscc/tips-for-making-effective-powerpoint-presentations\n",
    "    > - https://www.trentu.ca/academicskills/how-guides/how-write-university/how-approach-any-assignment/creating-effective-powerpoint-slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Cristian Camilo Zapata Zuluaga.\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rafael Juarez Badillo Chavez, Xander Gallegos, Javier Figueroa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
